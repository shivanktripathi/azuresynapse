{
	"name": "ApprovalHistoryV2",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "LMTSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b1a52a9c-b9bd-49c1-be6b-5f30d177a33f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "scala"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/a2d99d65-e76e-4ea1-90fa-63c34843c7a0/resourceGroups/RG-LMT-CORE-UAT/providers/Microsoft.Synapse/workspaces/labormangementsynapseppe/bigDataPools/LMTSparkPool",
				"name": "LMTSparkPool",
				"type": "Spark",
				"endpoint": "https://labormangementsynapseppe.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/LMTSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"Drop TABLE IF EXISTS tbl_LaborTransactionAudit;\r\n",
					"CREATE TABLE tbl_LaborTransactionAudit USING cosmos.olap OPTIONS (\r\n",
					"\tspark.synapse.linkedService 'lmt_cosmos_prod'\r\n",
					"\t,spark.cosmos.container 'LaborTransactionAudit'\r\n",
					"\t)"
				],
				"attachments": null,
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.conf.set(\"fs.azure.account.key.lmtsalrscoreuateastus.blob.core.windows.net\",\"hCAr724m+dDe3QpkKrMDvKvqf3dcQy25nUL9AjJyDU66To6NQcDUgkQ57smWqatNKpZ1Ji4tFhGqCd7UMpuQng==\")"
				],
				"attachments": null,
				"execution_count": 130
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"var df = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\")"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"var df = spark.sql(\"\"\"SELECT labor.laborId,\r\n",
					"                             labor.laborDate,\r\n",
					"                             labor.laborHours,\r\n",
					"                             labor.laborTimeZoneId,\r\n",
					"                             labor.laborCategoryId,\r\n",
					"                             labor.submittedFor,\r\n",
					"                             labor.submittedBy,\r\n",
					"                             labor.submittedDateInUtc,\r\n",
					"                             labor.laborStatus,\r\n",
					"                             labor.laborNotes,\r\n",
					"                             labor.partner,\r\n",
					"                             labor.assignmentDetails.assignmentId,\r\n",
					"                             labor.updatedDateTimeInUtc,\r\n",
					"                             id FROM tbl_LaborTransactionAudit WHERE labor.laborDate > date_add(CURRENT_DATE(), -366) and labor.laborStatus = 'Approved' \"\"\")\r\n",
					"\r\n",
					""
				],
				"attachments": null,
				"execution_count": 131
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"//display(df)"
				],
				"attachments": null,
				"execution_count": 126
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"//val outputfile = \"wasbs://approvalhistory@lmtsalrscoreuateastus.blob.core.windows.net/\"  \r\n",
					"//var filename = \"approval_history.csv\"\r\n",
					"//var outputFileName = outputfile +  filename \r\n",
					"\r\n",
					"//var output_container_name = \"approvalhistory\"\r\n",
					"//var storage_name = \"lmtsalrscoreuateastus\"\r\n",
					"var output_container_path = \"wasbs://approvalhistory@lmtsalrscoreuateastus.blob.core.windows.net/\"\r\n",
					"var output_blob_folder = output_container_path + \"/wrangled_data_folder\"\r\n",
					"\r\n",
					"\r\n",
					"df.coalesce(1).write.format(\"json\").option(\"header\", \"true\").mode(\"overwrite\").save(output_blob_folder)\r\n",
					""
				],
				"attachments": null,
				"execution_count": 132
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.dbutils import DBUtils\r\n",
					"\r\n",
					"output_container_path = \"wasbs://approvalhistory@lmtsalrscoreuateastus.blob.core.windows.net/\"\r\n",
					"output_blob_folder = output_container_path + \"/wrangled_data_folder\"\r\n",
					"\r\n",
					"files = dbutils.fs.ls(output_blob_folder)\r\n",
					"output_file = [x for x in files if x.name.startswith(\"part-\")]\r\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"val df1 = spark.read.json(\"wasbs://approvalhistory@lmtsalrscoreuateastus.blob.core.windows.net//wrangled_data_folder\")\r\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(df1.limit(400))"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "csharp"
					}
				},
				"source": [
					"%%csharp\r\n",
					"\r\n",
					"#r \"nuget: Azure.Data.Tables, 12.3.0\"\r\n",
					"\r\n",
					"using Microsoft.Spark.Sql;\r\n",
					"using Microsoft.Spark.Sql.Types;\r\n",
					"using System.Collections.Generic;\r\n",
					"\r\n",
					"\r\n",
					"string filePath = $\"wasbs://approvalhistory@lmtsalrscoreuateastus.blob.core.windows.net//wrangled_data_folder\";\r\n",
					"\r\n",
					"SparkSession spark = SparkSession\r\n",
					"    .Builder()\r\n",
					"    .AppName(\"Azure Storage example using .NET for Apache Spark\")\r\n",
					"    .Config(\"fs.wasbs.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\r\n",
					"    .Config($\"fs.azure.account.key.lmtsalrscoreuateastus.blob.core.windows.net\", \"hCAr724m+dDe3QpkKrMDvKvqf3dcQy25nUL9AjJyDU66To6NQcDUgkQ57smWqatNKpZ1Ji4tFhGqCd7UMpuQng==\")\r\n",
					"    .GetOrCreate();\r\n",
					"\r\n",
					"\r\n",
					"DataFrame readDf = spark.Read().Json(filePath);\r\n",
					"readDf.Show();"
				],
				"attachments": null,
				"execution_count": 128
			}
		]
	}
}